# Обзор на различные векторные представления слов

## Bag of words

Модель представления текста, которая заключается в отображении набора слов, входящих в определенный документ без учета их взаимосвязей. 

### Пошаговый пример обработки

1. Удалить лишние данные (HTML, XML разметку и т.п.);
2. Нормализация (привести всё к строчным буквам, избавиться от пунктуации и знаков);
3. Токенизация слов текста;
4. Выделить уникальные слова из всех документов и представить их в виде координат вектора, где каждая координата отвечает за одно слово;
5. Для каждого документа определить количество вхождений каждого слова и записать это значение в соответствующую координату. Получается искомый вектор документа.

Стоит отметить, что можно использовать *n-gram model*, которая учитывать несколько подряд идущих слов.

### Достоинства 

1. Относительно простой способ;
2. Легко сравнивать документы.

### Недостатки 

1. Итоговый результат - матрица, состоящая из векторов документов. Матрица является разреженной, что вынуждает приводить её к плотной;
2. Теряется порядок и взаимосвязь слов; 
3. Долгое время исполнения;

## TF-IDF

Показатель, оценивающий важность термина для одного текста относительно всех текстов.

### Способ вычисления

1. Рассчитать TF = (количество вхождений термина в документов)/(количество слов в документе)
2. Рассчитать IDF = ln((количество всех документов)/(количество документов, в которых встречается термин))
3. TF-IDF = TF * IDF

### Достоинства

1. Простота вычисления;
2. Простой способ сравнить документы;
3. Хорошая начальная метрика.

### Недостатки

1. Не учитывает семантику слов, позицию в предложении и тд;
2. Невозможно использовать анализ текста выше лексического уровня.

## Word2vec

Технология, позволяющая собрать текстовые данные и расположить слова в векторном пространстве так, чтобы в максимальной степени отразить отношения этих слов в обрабатываемых текстах (семантическая близость).
Векторы слов сравниваются по *косинусной близости*.

### СBOW и Skip Gram

Существуют две модели реализации word2vec: СBOW и Skip Gram.
Skip gram работает лучше с небольшим количеством данных и редкими словами.
CBOW работает быстрее и показывает лучший результат с часто встречающимися словами в тексте.

### Общий план использования

1. Настроить данные;
2. Обучить модель;
3. Использовать модель;
4. Profit!

### Достоинства

1. Простота использования;
2. Возможность регулировать параметры обучения;
3. Можно загружать данные онлайн. 

### Недостатки

1. Необходим большой корпус для обучения;
2. Отношения слов в векторном пространстве не совсем очевидны и требуют интерпретации.

## Заключение

Рассмотренные выше векторые представления слов решают различные задачи:
	- Bag of words - анализ текста
	- TF-IDF - определение важных теримнов
	- Word2vec - определение семантический отноншений слов
Все эти модели несложны и помогают добиться качественного анализа текста.