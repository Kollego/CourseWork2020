{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize, regexp_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pymorphy2\n",
    "\n",
    "from natasha import NamesExtractor\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Открываем необходимые бинарники"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = '..\\\\05. Main model\\\\03. Corr matrix creation\\\\Pickles'\n",
    "\n",
    "with open(join(path1, 'corr_matrix.pickle'), 'rb') as data:\n",
    "    corr_matrix = pickle.load(data)\n",
    "    \n",
    "with open(join(path1, 'groups_in_matrix.pickle'), 'rb') as data:\n",
    "    groups_in_matrix = pickle.load(data)\n",
    "    \n",
    "with open(join(path1, 'vectorizer.pickle'), 'rb') as data:\n",
    "    bof = pickle.load(data)\n",
    "    \n",
    "path2 = '..\\\\05. Main model\\\\04. Feature engineering\\\\Pickles'  \n",
    "\n",
    "with open(join(path2, 'tfidf.pickle'), 'rb') as data:\n",
    "    tfidf = pickle.load(data)\n",
    "\n",
    "path3 = '..\\\\05. Main model\\\\05. Model training\\\\Pickles'\n",
    "\n",
    "with open(join(path3, 'svc_classifiers.pickle'), 'rb') as data:\n",
    "    svc_classifiers = pickle.load(data)\n",
    "    \n",
    "with open(join(path3, 'rfc_classifiers.pickle'), 'rb') as data:\n",
    "    rfc_classifiers = pickle.load(data)\n",
    "    \n",
    "with open(join(path3, 'svc_named_entity.pickle'), 'rb') as data:\n",
    "    svc_named_entity = pickle.load(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создаем функции для обработки текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_stopwords_ru = \"https://raw.githubusercontent.com/stopwords-iso/stopwords-ru/master/stopwords-ru.txt\"\n",
    "\n",
    "\n",
    "def get_text(url, encoding='utf-8', to_lower=True):\n",
    "    url = str(url)\n",
    "    if url.startswith('http'):\n",
    "        r = requests.get(url)\n",
    "        if not r.ok:\n",
    "            r.raise_for_status()\n",
    "        return r.text.lower() if to_lower else r.text\n",
    "    elif os.path.exists(url):\n",
    "        with open(url, encoding=encoding) as f:\n",
    "            return f.read().lower() if to_lower else f.read()\n",
    "    else:\n",
    "        raise Exception('parameter [url] can be either URL or a filename')\n",
    "\n",
    "\n",
    "def normalize_tokens(tokens):\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    return [morph.parse(tok)[0].normal_form for tok in tokens]\n",
    "\n",
    "\n",
    "def remove_stopwords(tokens, stopwords=None, min_length=4):\n",
    "    if not stopwords:\n",
    "        return tokens\n",
    "    stopwords = set(stopwords)\n",
    "    tokens = [tok\n",
    "              for tok in tokens\n",
    "              if tok not in stopwords and len(tok) >= min_length]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def tokenize_n_lemmatize(text, stopwords=None, normalize=True, regexp=r'(?u)\\b\\w{4,}\\b'):\n",
    "    words = [w for sent in sent_tokenize(text)\n",
    "             for w in regexp_tokenize(sent, regexp)]\n",
    "    if normalize:\n",
    "        words = normalize_tokens(words)\n",
    "    if stopwords:\n",
    "        words = remove_stopwords(words, stopwords)\n",
    "    return ' '.join(words)\n",
    "\n",
    "stopwords_ru = get_text(url_stopwords_ru).splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создаем необходимые функции для анализа текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# парсинг текста, приведение в нормальную форму\n",
    "def parse_text(text):\n",
    "    return tokenize_n_lemmatize(text)\n",
    "\n",
    "# создание вектора tfidf  \n",
    "def make_tfidf_from_text(text_parsed):\n",
    "    vect = tfidf.transform([text_parsed]).toarray()\n",
    "    return vect\n",
    "\n",
    "# создание bag of words из направления сочинения\n",
    "def make_bof_from_direction(direction):\n",
    "    direction_parsed = tokenize_n_lemmatize(direction)\n",
    "    vect = bof.transform([direction_parsed]).toarray()\n",
    "    return vect\n",
    "\n",
    "# предсказание вероятности, что текст принадлежит определенной теме, с помощью svc\n",
    "def make_text_prediction_svc(text):\n",
    "    prediction = []\n",
    "    text_parsed = parse_text(text)\n",
    "    text_tfidf = make_tfidf_from_text(text_parsed)\n",
    "    for svc in svc_classifiers:\n",
    "        prediction.append(svc.predict_proba(text_tfidf)[0][1])\n",
    "    return prediction\n",
    "\n",
    "# предсказание вероятности, что текст принадлежит определенной теме, с помощью rfc\n",
    "def make_text_prediction_rfc(text):\n",
    "    prediction = []\n",
    "    text_parsed = parse_text(text)\n",
    "    text_tfidf = make_tfidf_from_text(text_parsed)\n",
    "    for rfc in rfc_classifiers:\n",
    "        prediction.append(rfc.predict_proba(text_tfidf)[0][1])\n",
    "    return prediction\n",
    "\n",
    "# создание направления сочинения\n",
    "def make_direction_vector(prediction=None, threshold = 0.12, direction=None, topic=None):\n",
    "    k = len(prediction)\n",
    "    direction_vect = np.array([0] * k)\n",
    "    if prediction is not None:\n",
    "        indices = np.argsort(prediction)[-5:] \n",
    "        for i in indices:\n",
    "            if prediction[i] >= threshold:\n",
    "                direction_vect[i] = 1\n",
    "    if direction is not None:\n",
    "        pass\n",
    "    if topic is not None:\n",
    "        pass\n",
    "    return direction_vect\n",
    "\n",
    "# создание вектора именованных сущностей    \n",
    "def make_named_entity_vector(enteties):\n",
    "    ne_vect = np.array([0] * len(groups_in_matrix))\n",
    "    for _, _, g in enteties:\n",
    "        ne_vect[g] = 1\n",
    "    return ne_vect\n",
    "\n",
    "def make_named_entity_fit(direction_vect):\n",
    "    return direction_vect.dot(corr_matrix.T)\n",
    "\n",
    "# проверка того, хорошее ли сочинение\n",
    "def make_conclusion_for_writing(ne_vect, ne_fit):\n",
    "    X = np.multiply(ne_vect, ne_fit)\n",
    "    return svc_named_entity.predict(np.array([X]))[0]\n",
    "\n",
    "# получение индексы альтернативных именованных сущностей, подходящих тексту\n",
    "def get_alternative_named_enteties(ne_fit, threshold = 0.5):\n",
    "    m = len(ne_fit)\n",
    "    result = []\n",
    "    indices = np.argsort(ne_fit)[-5:]\n",
    "    for i in indices:\n",
    "        if ne_fit[i] >= threshold:\n",
    "            result.append(i)\n",
    "    return result\n",
    "\n",
    "def get_longest_ne(group):\n",
    "    max_l = 0\n",
    "    l_el = ''\n",
    "    for el in group:\n",
    "        m = re.search('[\\\"](.+)[\\\"]', el)\n",
    "        if m is not None:\n",
    "            return '\\'{}\\''.format(m.group(1))\n",
    "        if len(el) > max_l:\n",
    "            l_el = el\n",
    "            max_l = len(el)\n",
    "            \n",
    "    return l_el.title()\n",
    "\n",
    "# Функция расчета схожести строк по коэффициенту Жаккара\n",
    "def dist_jaccard(string1, string2):\n",
    "    \n",
    "    str1 = string1\n",
    "    str2= string2\n",
    "    a = len(str1)\n",
    "    b = len(str2)\n",
    "    c = 0\n",
    "    \n",
    "    while str1 != '':\n",
    "        \n",
    "        s = str1[0]\n",
    "        r = re.compile(s)\n",
    "        c += min(len(r.findall(str1)), len(r.findall(str2)))\n",
    "        str1 = r.sub('', str1)\n",
    "        str2 = r.sub('', str2)\n",
    "        \n",
    "    return c / (a + b - c)\n",
    "\n",
    "# Сравнение двух именованных сущностей\n",
    "def compare_str(str1, str2):\n",
    "    s1 = str1.split()\n",
    "    s2 = str2.split()\n",
    "    if len(s1) < len(s2):\n",
    "        s1, s2 = s2, s1\n",
    "    n = len(s1)\n",
    "    m = len(s2)\n",
    "    result = False\n",
    "    b1 = re.search('[\\\"](.+)[\\\"]', str1)\n",
    "    b2 = re.search('[\\\"](.+)[\\\"]', str2)\n",
    "    if b1 is not None and b2 is not None:\n",
    "        if b1.group(1) == b2.group(1):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    elif b1 is not None or b2 is not None:\n",
    "        return False\n",
    "    if n == 3:\n",
    "        if m == 3:\n",
    "            if s1[0] == s2[0] and s1[1] == s2[1] and s1[2] == s2[2]:\n",
    "                result = True\n",
    "            elif s1[0][0] == s2[0][0] and s1[1][0] == s2[1][0] and s1[2] == s2[2] and (len(s1[0]) == 1 or len(s2[0]) == 1):\n",
    "                result = True\n",
    "        elif m == 2:\n",
    "            if s1[0] == s2[0] and s1[2] == s2[1]:\n",
    "                result = True\n",
    "            elif s1[0][0] == s2[0][0] and s1[2] == s2[1] and (len(s1[0]) == 1 or len(s2[0]) == 1):\n",
    "                result = True\n",
    "        else:\n",
    "            if s1[2] == s2[0]:\n",
    "                result = True\n",
    "    elif n == 2:\n",
    "        if m == 2:\n",
    "            if s1[0] == s2[0] and s1[1] == s2[1]:\n",
    "                result = True\n",
    "            elif s1[0][0] == s2[0][0] and s1[1] == s2[1] and (len(s1[0]) == 1 or len(s2[0]) == 1):\n",
    "                result = True\n",
    "        else:\n",
    "            if s1[1] == s2[0]:\n",
    "                result = True\n",
    "    elif n == 1:\n",
    "        if s1[0] == s2[0]:\n",
    "            result = True\n",
    "    elif dist_jaccard(str1, str2) >= 0.9:\n",
    "        result = True\n",
    "    else:\n",
    "        result = False\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Функция сравнения двух групп именовааных сущностей\n",
    "def compare_with_group(name, group):\n",
    "    \n",
    "    for el in group:\n",
    "            if compare_str(name, el):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def get_group(name):\n",
    "    for i in range(len(groups_in_matrix)):\n",
    "        if compare_with_group(name, groups_in_matrix[i]):\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def extract_names(text):\n",
    "    \n",
    "    extractor = NamesExtractor()\n",
    "    matches = extractor(text)\n",
    "    result = []\n",
    "    pattern = re.compile('[\\\"\\«\\“](.+)[\\\"\\»\\”]')\n",
    "    \n",
    "    for match in matches:\n",
    "        \n",
    "        name = []\n",
    "        start, stop = match.span\n",
    "        \n",
    "        if match.fact.first != None:\n",
    "                name.append(match.fact.first)\n",
    "        if match.fact.middle != None:\n",
    "                name.append(match.fact.middle)\n",
    "        if match.fact.last != None:\n",
    "                name.append(match.fact.last)\n",
    "                \n",
    "        name = ' '.join(name).lower()\n",
    "        group = get_group(name)\n",
    "        \n",
    "        result.append((start, stop, group))\n",
    "        \n",
    "        res_regexp = pattern.search(text, max(start-75, 0), min(stop+75, len(text)))\n",
    "        if res_regexp is not None:\n",
    "            book = name + ' \\\"' + res_regexp.group(1) + '\\\"'\n",
    "            book_start, book_stop = res_regexp.span()\n",
    "            book_group = get_group(book)\n",
    "            if book_group != -1:\n",
    "                result.append((book_start, book_stop, book_group))\n",
    "            \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка работоспособонсти на реальных сочинениях"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    # Получаем именованные сущности\n",
    "    named_enteties = extract_names(text)\n",
    "    \n",
    "    # Превращаем именованные сущности в вектор\n",
    "    ne_vect = make_named_entity_vector(named_enteties)\n",
    "\n",
    "    # Прогнозируем вероятности принадлежности к каждой теме\n",
    "    prediction = make_text_prediction_rfc(text)\n",
    "    \n",
    "    # Из прогноза делаем вектор тем\n",
    "    direction_vect = make_direction_vector(prediction)\n",
    "    \n",
    "    # Считаем, насколько бы подходили к данному тексту всевозможные именованные сущности\n",
    "    ne_fit = make_named_entity_fit(direction_vect)\n",
    "    \n",
    "    # Делаем вывод, хорошее ли сочинение\n",
    "    conclusion = make_conclusion_for_writing(ne_vect, ne_fit)\n",
    "    \n",
    "    print('Сочинение {}'.format(path))\n",
    "    print('\\nНаправления текста (их их вероятность 0-1): ')\n",
    "    for i in range(len(direction_vect)):\n",
    "        if direction_vect[i] == 1:\n",
    "            print('{} ({:.3f});'.format(bof.get_feature_names()[i], prediction[i]))\n",
    "    \n",
    "    print('\\nАргументы сочинения (и их уместность 0-1):')\n",
    "    for s,e,g in named_enteties:\n",
    "        if g == -1:\n",
    "            print('\\t{};'.format(text[s:e]))\n",
    "        else:\n",
    "            print('\\t{} ({:.3f});'.format(text[s:e], ne_fit[g]))\n",
    "    \n",
    "    print('\\nВозможные альтернативные аргументы для данного текста:')\n",
    "    alt_ne = get_alternative_named_enteties(ne_fit)\n",
    "    for ne_i in alt_ne:\n",
    "        group = groups_in_matrix[ne_i]\n",
    "        print('\\t{};'.format(get_longest_ne(group)))\n",
    "                  \n",
    "    print('\\nХорошее ли сочинение: ')\n",
    "    if conclusion == 1:\n",
    "        print('Да')\n",
    "    else:\n",
    "        print('Нет')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сочинение text_examples\\text1.txt\n",
      "\n",
      "Направления текста (их их вероятность 0-1): \n",
      "боль (0.200);\n",
      "добро (0.255);\n",
      "жизнь (0.221);\n",
      "зло (0.177);\n",
      "тепло (0.183);\n",
      "\n",
      "Аргументы сочинения (и их уместность 0-1):\n",
      "\tМ.Уилсона;\n",
      "\tЛ. Куликовой;\n",
      "\tМопассан;\n",
      "\tБ. Васильева (-0.143);\n",
      "\n",
      "Возможные альтернативные аргументы для данного текста:\n",
      "\t'Чудесный доктор';\n",
      "\tМихаил Роман;\n",
      "\tЛизавета;\n",
      "\t'Дочь Бухары';\n",
      "\tЛюдмила Улицкая;\n",
      "\n",
      "Хорошее ли сочинение: \n",
      "Да\n"
     ]
    }
   ],
   "source": [
    "report('text_examples\\\\text1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сочинение text_examples\\text2.txt\n",
      "\n",
      "Направления текста (их их вероятность 0-1): \n",
      "гордость (0.655);\n",
      "достоинство (0.577);\n",
      "сила (0.598);\n",
      "характер (0.602);\n",
      "честь (0.582);\n",
      "\n",
      "Аргументы сочинения (и их уместность 0-1):\n",
      "\tМ.Ю. Лермонтова (0.428);\n",
      "\t«Герой нашего времени» (0.421);\n",
      "\tГригория Печорина (0.408);\n",
      "\tПечорин (0.408);\n",
      "\tМери (0.681);\n",
      "\tМихаила Шолохова (0.576);\n",
      "\t«Судьба человека» (0.892);\n",
      "\n",
      "Возможные альтернативные аргументы для данного текста:\n",
      "\tМери;\n",
      "\tЭлен Курагина;\n",
      "\tПьер Безухов;\n",
      "\t'Судьба человека';\n",
      "\tМария;\n",
      "\n",
      "Хорошее ли сочинение: \n",
      "Да\n"
     ]
    }
   ],
   "source": [
    "report('text_examples\\\\text2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сочинение text_examples\\text3.txt\n",
      "\n",
      "Направления текста (их их вероятность 0-1): \n",
      "война (0.184);\n",
      "добро (0.156);\n",
      "жизнь (0.267);\n",
      "надежда (0.194);\n",
      "характер (0.145);\n",
      "\n",
      "Аргументы сочинения (и их уместность 0-1):\n",
      "\tН.Евдокимова;\n",
      "\tА. Качалов;\n",
      "\n",
      "Возможные альтернативные аргументы для данного текста:\n",
      "\tНиколай;\n",
      "\tАндрей Болконский;\n",
      "\tЛюдмила Улицкая;\n",
      "\t'Дочь Бухары';\n",
      "\n",
      "Хорошее ли сочинение: \n",
      "Да\n"
     ]
    }
   ],
   "source": [
    "report('text_examples\\\\text3.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
